{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import itertools\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from weather_data_pipeline import WeatherPipeline, DatasetConfig\n",
    "from weather_feature_pipeline import FeaturePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorfy(x):\n",
    "    if torch.is_tensor(x):\n",
    "        return x.clone().detach().float()\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size_temp, output_size_wind):\n",
    "        super(WeatherModel, self).__init__()\n",
    "        \n",
    "        layer_sizes = [input_size] + list(hidden_sizes)\n",
    "        layers = []\n",
    "        \n",
    "        # Create layers with proper initialization\n",
    "        for in_size, out_size in itertools.pairwise(layer_sizes):\n",
    "            linear = nn.Linear(in_size, out_size)\n",
    "            # Use He initialization for ReLU\n",
    "            # nn.init.kaiming_uniform_(linear.weight, nonlinearity='relu')\n",
    "            # nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            layers.extend([\n",
    "                linear,\n",
    "                # nn.BatchNorm1d(out_size),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "        \n",
    "        self.fc1 = nn.Sequential(*layers)\n",
    "        \n",
    "        # Temperature output (regression)\n",
    "        self.fc2_temp = nn.Linear(layer_sizes[-1], output_size_temp)\n",
    "        # nn.init.xavier_uniform_(self.fc2_temp.weight)\n",
    "        # nn.init.zeros_(self.fc2_temp.bias)\n",
    "        \n",
    "        # Wind output (binary classification)\n",
    "        self.fc2_wind = nn.Linear(layer_sizes[-1], output_size_wind)\n",
    "        # nn.init.xavier_uniform_(self.fc2_wind.weight)\n",
    "        # nn.init.zeros_(self.fc2_wind.bias)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = tensorfy(x)\n",
    "        out = self.fc1(x)\n",
    "        temp = self.fc2_temp(out)\n",
    "        wind = self.sigmoid(self.fc2_wind(out))\n",
    "        return temp.squeeze(), wind.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to stop the training when the loss does not improve.\n",
    "    This is to prevent overfitting and save the best model.\n",
    "    \n",
    "    Args:\n",
    "        patience (int): How long to wait after last time validation loss improved.\n",
    "        verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "        delta (float): Minimum change in monitored quantity to qualify as an improvement.\n",
    "        path (str): Path for the checkpoint to be saved to.\n",
    "        trace_func (callable): trace print function.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        patience: int = 7,\n",
    "        verbose: bool = False,\n",
    "        delta: float = 0,\n",
    "        path: str = 'checkpoint.pt',\n",
    "        trace_func = print\n",
    "    ):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, val_loss: float, model: torch.nn.Module) -> bool:\n",
    "        score = -val_loss  # We want to maximize the negative loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "    \n",
    "    def save_checkpoint(self, val_loss: float, model: torch.nn.Module):\n",
    "        \"\"\"Save model when validation loss decrease.\"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        \n",
    "        # Save a deep copy of the model state\n",
    "        self.best_model = copy.deepcopy(model)\n",
    "        if self.path:\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "    \n",
    "    def load_best_model(self) -> torch.nn.Module:\n",
    "        \"\"\"Return the best model found during training.\"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise RuntimeError(\"No model has been saved yet.\")\n",
    "        return self.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_config(optimizer_name: str = 'sgd'):\n",
    "    \"\"\"Get optimizer configuration based on optimizer name.\n",
    "    \n",
    "    Args:\n",
    "        optimizer_name: Either 'sgd' or 'adam'\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimizer, scheduler, optimizer_params)\n",
    "    \"\"\"\n",
    "    base_config = {\n",
    "        'weight_decay': 1e-4,  # L2 regularization for both optimizers\n",
    "    }\n",
    "    \n",
    "    if optimizer_name.lower() == 'sgd':\n",
    "        optimizer_class = optim.SGD\n",
    "        optimizer_params = {\n",
    "            **base_config,\n",
    "            'lr': 0.001,\n",
    "            'momentum': 0.9,\n",
    "            'nesterov': True,\n",
    "        }\n",
    "    elif optimizer_name.lower() == 'adam':\n",
    "        optimizer_class = optim.Adam\n",
    "        optimizer_params = {\n",
    "            **base_config,\n",
    "            'lr': 0.001,\n",
    "            'betas': (0.9, 0.999),\n",
    "            'eps': 1e-8,\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    \n",
    "    def create_optimizer(params):\n",
    "        optimizer = optimizer_class(params, **optimizer_params)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=10\n",
    "        )\n",
    "        return optimizer, scheduler\n",
    "    \n",
    "    return create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, feature_pipeline, train_data, val_data,\n",
    "               criterion_temp, criterion_wind, optimizer, scheduler=None,\n",
    "               num_epochs=10_000, patience=20, scale_features=False):\n",
    "    \n",
    "    X_train = train_data[\"X\"]\n",
    "    X_val = val_data[\"X\"]\n",
    "    if scale_features:\n",
    "        X_train = feature_pipeline.scale_features(X_train, feature_names)\n",
    "        X_val = feature_pipeline.scale_features(X_val, feature_names)\n",
    "    \n",
    "    X_train = tensorfy(X_train).to(device)\n",
    "    X_val = tensorfy(X_val).to(device)\n",
    "    y_temp_train = tensorfy(train_data[\"y_temp\"]).to(device)\n",
    "    y_wind_train = tensorfy(train_data[\"y_wind\"]).to(device)\n",
    "    y_temp_val = tensorfy(val_data[\"y_temp\"]).to(device)\n",
    "    y_wind_val = tensorfy(val_data[\"y_wind\"]).to(device)\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'learning_rates': []}\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=False, delta=1e-4)\n",
    "    \n",
    "    for epoch in (pbar := tqdm(range(num_epochs), desc='Training')):\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs_temp, outputs_wind = model(X_train)\n",
    "        \n",
    "        if scale_features:\n",
    "            unscaled_temp_pred = feature_pipeline.inverse_scale_features(outputs_temp.detach().cpu().numpy(), feature_type=\"temperature\")\n",
    "            unscaled_temp_pred = tensorfy(unscaled_temp_pred).to(outputs_temp.device)\n",
    "            loss_temp = criterion_temp(unscaled_temp_pred, y_temp_train)\n",
    "        else:\n",
    "            loss_temp = criterion_temp(outputs_temp, y_temp_train)\n",
    "            \n",
    "        loss_wind = criterion_wind(outputs_wind, y_wind_train)\n",
    "        loss = loss_temp + loss_wind\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs_temp, val_outputs_wind = model(X_val)\n",
    "            \n",
    "            if scale_features:\n",
    "                unscaled_val_temp = feature_pipeline.inverse_scale_features(val_outputs_temp.cpu().numpy(), feature_type=\"temperature\")\n",
    "                unscaled_val_temp = tensorfy(unscaled_val_temp).to(val_outputs_temp.device)\n",
    "                val_loss_temp = criterion_temp(unscaled_val_temp, y_temp_val)\n",
    "            else:\n",
    "                val_loss_temp = criterion_temp(val_outputs_temp, y_temp_val)\n",
    "                \n",
    "            val_loss_wind = criterion_wind(val_outputs_wind, y_wind_val)\n",
    "            val_loss = val_loss_temp + val_loss_wind\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        history['train_loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            pbar.set_description(\n",
    "                f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                f'Loss: {loss.item():.4f}, '\n",
    "                f'Val Loss: {val_loss.item():.4f}, '\n",
    "                f'LR: {current_lr:.6f}'\n",
    "            )\n",
    "        \n",
    "        if early_stopping(val_loss.item(), model):\n",
    "            print(f'Early stopping triggered at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    model = early_stopping.load_best_model()\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(train_dataset: dict[str, dict[str, np.ndarray]], \n",
    "                          val_dataset: dict[str, dict[str, np.ndarray]],\n",
    "                          optimizer_name: str = 'sgd',\n",
    "                          feature_pipeline = None,\n",
    "                          scale_features: bool = False,\n",
    "                          n_epochs: int = 10_000):\n",
    "    \"\"\"Create and train weather prediction model.\"\"\"\n",
    "    # Create model\n",
    "    model = WeatherModel(\n",
    "        input_size=train_dataset['X'].shape[1],\n",
    "        hidden_sizes=(32,),\n",
    "        output_size_temp=1,\n",
    "        output_size_wind=1\n",
    "    ).to(device)\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_temp = nn.MSELoss()\n",
    "    criterion_wind = nn.BCELoss()\n",
    "\n",
    "    # Get optimizer configuration\n",
    "    optimizer_creator = get_optimizer_config(optimizer_name)\n",
    "    optimizer, scheduler = optimizer_creator(model.parameters())\n",
    "\n",
    "    # Train model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        feature_pipeline=feature_pipeline,\n",
    "        train_data=train_dataset,\n",
    "        val_data=val_dataset,\n",
    "        criterion_temp=criterion_temp,\n",
    "        criterion_wind=criterion_wind,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=n_epochs,\n",
    "        patience=20,\n",
    "        scale_features=scale_features\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, feature_pipeline, input_data: np.ndarray, feature_names: List[str], scale_features: bool = False) -> tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    \n",
    "    X = input_data\n",
    "    if scale_features:\n",
    "        X = feature_pipeline.scale_features(X, feature_names)\n",
    "    \n",
    "    X = tensorfy(X).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        temp_pred, wind_pred = model(X)\n",
    "        temp_pred = temp_pred.cpu().numpy()\n",
    "        if scale_features:\n",
    "            temp_pred = feature_pipeline.inverse_scale_features(temp_pred)\n",
    "        return temp_pred, wind_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.log(history['train_loss']), label='Training Loss')\n",
    "    plt.plot(np.log(history['val_loss']), label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_temp(city: str, y_temp_test_np: np.ndarray, outputs_temp_test_np: np.ndarray):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(y_temp_test_np, outputs_temp_test_np, alpha=0.5, label='Test Samples')\n",
    "    plt.plot([y_temp_test_np.min(), y_temp_test_np.max()], \n",
    "             [y_temp_test_np.min(), y_temp_test_np.max()], \n",
    "             'r--', label='Ideal Line')\n",
    "    plt.xlabel('Actual Temperature (째C)')\n",
    "    plt.ylabel('Predicted Temperature (째C)')\n",
    "    plt.title(f'Actual vs Predicted Temperature for {city} - Test')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_temp_in_time(city: str, y_temp_test_np: np.ndarray, outputs_temp_test_np: np.ndarray):\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(y_temp_test_np, label='Actual Temperature', color='blue')\n",
    "    plt.plot(outputs_temp_test_np, label='Predicted Temperature', color='orange')\n",
    "\n",
    "    mae = np.mean(np.abs(y_temp_test_np - outputs_temp_test_np))\n",
    "\n",
    "    differences = np.abs(y_temp_test_np - outputs_temp_test_np)\n",
    "    mask_high_diff = differences > 2\n",
    "    plt.scatter(np.where(mask_high_diff)[0], y_temp_test_np[mask_high_diff],\n",
    "                color='red', label='Difference > 2째C', zorder=5)\n",
    "\n",
    "    plt.xlabel('Test Samples')\n",
    "    plt.ylabel('Temperature (째C)')\n",
    "    plt.title(f'Actual vs Predicted Temperature Over Time for {city} - Test - MAE: {mae:.2f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_weather_prediction(city: str, city_data: dict, feature_pipeline, scale_features: bool = False, optimizer_name: str = \"sgd\", n_epochs: int = 10_000):\n",
    "    model, history = create_and_train_model(\n",
    "        train_dataset=city_data[\"train\"],\n",
    "        val_dataset=city_data[\"val\"],\n",
    "        optimizer_name=optimizer_name,\n",
    "        feature_pipeline=feature_pipeline,\n",
    "        scale_features=scale_features,\n",
    "        n_epochs=n_epochs\n",
    "    )\n",
    "    \n",
    "    temp_pred, wind_pred = predict(\n",
    "        model=model,\n",
    "        feature_pipeline=feature_pipeline,\n",
    "        input_data=city_data[\"test\"][\"X\"],\n",
    "        scale_features=scale_features\n",
    "    )\n",
    "    \n",
    "    plot_losses(history)\n",
    "    compare_temp(city, city_data[\"test\"][\"y_temp\"], temp_pred)\n",
    "    plot_temp_in_time(city, city_data[\"test\"][\"y_temp\"], temp_pred)\n",
    "    \n",
    "    return model, history, (temp_pred, wind_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-windowed data for Albuquerque...\n",
      "Loading pre-windowed data for Atlanta...\n",
      "Loading pre-windowed data for Beersheba...\n",
      "Loading pre-windowed data for Boston...\n",
      "Loading pre-windowed data for Charlotte...\n",
      "Loading pre-windowed data for Chicago...\n",
      "Loading pre-windowed data for Dallas...\n",
      "Loading pre-windowed data for Denver...\n",
      "Loading pre-windowed data for Detroit...\n",
      "Loading pre-windowed data for Eilat...\n",
      "Loading pre-windowed data for Haifa...\n",
      "Loading pre-windowed data for Houston...\n",
      "Loading pre-windowed data for Indianapolis...\n",
      "Loading pre-windowed data for Jacksonville...\n",
      "Loading pre-windowed data for Jerusalem...\n",
      "Loading pre-windowed data for Kansas City...\n",
      "Loading pre-windowed data for Las Vegas...\n",
      "Loading pre-windowed data for Los Angeles...\n",
      "Loading pre-windowed data for Miami...\n",
      "Loading pre-windowed data for Minneapolis...\n",
      "Loading pre-windowed data for Montreal...\n",
      "Loading pre-windowed data for Nahariyya...\n",
      "Loading pre-windowed data for Nashville...\n",
      "Loading pre-windowed data for New York...\n",
      "Loading pre-windowed data for Philadelphia...\n",
      "Loading pre-windowed data for Phoenix...\n",
      "Loading pre-windowed data for Pittsburgh...\n",
      "Loading pre-windowed data for Portland...\n",
      "Loading pre-windowed data for Saint Louis...\n",
      "Loading pre-windowed data for San Antonio...\n",
      "Loading pre-windowed data for San Diego...\n",
      "Loading pre-windowed data for San Francisco...\n",
      "Loading pre-windowed data for Seattle...\n",
      "Loading pre-windowed data for Tel Aviv District...\n",
      "Loading pre-windowed data for Toronto...\n",
      "Loading pre-windowed data for Vancouver...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 8 features, but StandardScaler is expecting 1 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m city_datasets \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mprepare_datasets(config\u001b[38;5;241m=\u001b[39mconfig, load_windowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m city \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVancouver\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mrun_weather_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcity_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mrun_weather_prediction\u001b[1;34m(city, city_data, feature_pipeline, scale_features, optimizer_name, n_epochs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_weather_prediction\u001b[39m(city: \u001b[38;5;28mstr\u001b[39m, city_data: \u001b[38;5;28mdict\u001b[39m, feature_pipeline, scale_features: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, optimizer_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_epochs: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10_000\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcity_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcity_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     temp_pred, wind_pred \u001b[38;5;241m=\u001b[39m predict(\n\u001b[0;32m     12\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     13\u001b[0m         feature_pipeline\u001b[38;5;241m=\u001b[39mfeature_pipeline,\n\u001b[0;32m     14\u001b[0m         input_data\u001b[38;5;241m=\u001b[39mcity_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     15\u001b[0m         scale_features\u001b[38;5;241m=\u001b[39mscale_features\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     plot_losses(history)\n",
      "Cell \u001b[1;32mIn[8], line 25\u001b[0m, in \u001b[0;36mcreate_and_train_model\u001b[1;34m(train_dataset, val_dataset, optimizer_name, feature_pipeline, scale_features, n_epochs)\u001b[0m\n\u001b[0;32m     22\u001b[0m optimizer, scheduler \u001b[38;5;241m=\u001b[39m optimizer_creator(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_temp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_temp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion_wind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_wind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_features\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, feature_pipeline, train_data, val_data, criterion_temp, criterion_wind, optimizer, scheduler, num_epochs, patience, scale_features)\u001b[0m\n\u001b[0;32m      6\u001b[0m X_val \u001b[38;5;241m=\u001b[39m val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_features:\n\u001b[1;32m----> 8\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     X_val \u001b[38;5;241m=\u001b[39m feature_pipeline\u001b[38;5;241m.\u001b[39mscale_features(X_val, feature_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m X_train \u001b[38;5;241m=\u001b[39m tensorfy(X_train)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\weather_feature_pipeline.py:276\u001b[0m, in \u001b[0;36mFeaturePipeline.scale_features\u001b[1;34m(self, data, feature_type)\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_dataframe(data)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\weather_feature_pipeline.py:310\u001b[0m, in \u001b[0;36mFeaturePipeline._scale_array\u001b[1;34m(self, data, feature_type)\u001b[0m\n\u001b[0;32m    308\u001b[0m transformer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers[feature_type]\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transformer, StandardScalingTransformer):\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\weather_feature_pipeline.py:122\u001b[0m, in \u001b[0;36mStandardScalingTransformer.transform\u001b[1;34m(self, data, inverse)\u001b[0m\n\u001b[0;32m    120\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(values)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Restore original shape if input was not 2D\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1042\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1044\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1045\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\.venv\\Lib\\site-packages\\sklearn\\base.py:654\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 654\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Politechnika\\SN\\NN-project-3\\.venv\\Lib\\site-packages\\sklearn\\base.py:443\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 8 features, but StandardScaler is expecting 1 features as input."
     ]
    }
   ],
   "source": [
    "pipeline = WeatherPipeline()\n",
    "config = DatasetConfig(\n",
    "    test_size=0.15,\n",
    "    val_size=0.15,\n",
    "    window_size=3,\n",
    "    prediction_offset=1\n",
    ")\n",
    "\n",
    "# Prepare datasets (this will also load/initialize the feature pipeline)\n",
    "city_datasets = pipeline.prepare_datasets(config=config, load_windowed=True)\n",
    "\n",
    "city = \"Vancouver\"\n",
    "_ = run_weather_prediction(city, city_datasets[city], pipeline.feature_pipeline, scale_features=False, optimizer_name=\"sgd\", n_epochs=50000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
