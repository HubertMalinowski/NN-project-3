{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"./data\")\n",
    "\n",
    "file_dict: dict[str, dict[str, Path]]\n",
    "file_dict = {\n",
    "    \"scalar\": {\n",
    "        \"temperature\": (DATA_ROOT / \"temperature.csv\", (\"min\", \"max\")),\n",
    "        \"pressure\": (DATA_ROOT / \"pressure.csv\", (\"mean\",)),\n",
    "        \"humidity\": (DATA_ROOT / \"humidity.csv\", (\"mean\",)),\n",
    "        \"wind_speed\": (DATA_ROOT / \"wind_speed.csv\", (\"mean\", \"max\")),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_scalar_data(df: pd.DataFrame, measurement_name: str, agg_types: tuple, \n",
    "                         scaler_dir: str = 'scalers', is_training: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transform scalar weather data with aggregation and normalization.\n",
    "    Uses a single scaler for all statistics of the same measurement.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe with datetime and city columns\n",
    "        measurement_name: Name of the measurement (e.g., 'temperature')\n",
    "        agg_types: Tuple of aggregation types (e.g., ('min', 'max', 'mean'))\n",
    "        scaler_dir: Directory to save/load scalers\n",
    "        is_training: Whether this is training data (to fit scaler) or not\n",
    "    \"\"\"\n",
    "    # Create scaler directory if it doesn't exist\n",
    "    os.makedirs(scaler_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert datetime to datetime type if it isn't already\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Get list of city columns (all columns except datetime)\n",
    "    city_columns = [col for col in df.columns if col != 'datetime']\n",
    "    \n",
    "    # Melt the dataframe to convert from wide to long format\n",
    "    df_melted = df.melt(\n",
    "        id_vars=['datetime'],\n",
    "        value_vars=city_columns,\n",
    "        var_name='city',\n",
    "        value_name=measurement_name\n",
    "    )\n",
    "    \n",
    "    # Extract date from datetime\n",
    "    df_melted['date'] = df_melted['datetime'].dt.date\n",
    "    \n",
    "    # Group by date and city to calculate daily statistics\n",
    "    result = df_melted.groupby(['date', 'city']).agg({\n",
    "        measurement_name: [(f'{measurement_name}_{agg_type}', agg_type) for agg_type in agg_types]\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten multi-level columns\n",
    "    result.columns = ['date', 'city'] + [f'{measurement_name}_{agg_type}' for agg_type in agg_types]\n",
    "    \n",
    "    # Get columns to normalize (all except date and city)\n",
    "    cols_to_normalize = [f'{measurement_name}_{agg_type}' for agg_type in agg_types]\n",
    "    \n",
    "    # Use a single scaler for all statistics of this measurement\n",
    "    scaler_path = os.path.join(scaler_dir, f'{measurement_name}_scaler.pkl')\n",
    "    \n",
    "    if is_training:\n",
    "        # Fit and save scaler on all statistics together\n",
    "        scaler = StandardScaler()\n",
    "        result[cols_to_normalize] = scaler.fit_transform(result[cols_to_normalize])\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "    else:\n",
    "        # Load and use existing scaler\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "        result[cols_to_normalize] = scaler.transform(result[cols_to_normalize])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_wind_direction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert datetime to datetime type if it isn't already\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Get list of city columns (all columns except datetime)\n",
    "    city_columns = [col for col in df.columns if col != 'datetime']\n",
    "    \n",
    "    # Melt the dataframe\n",
    "    df_melted = df.melt(\n",
    "        id_vars=['datetime'],\n",
    "        value_vars=city_columns,\n",
    "        var_name='city',\n",
    "        value_name='wind_direction'\n",
    "    )\n",
    "    \n",
    "    # Convert degrees to radians and calculate x,y components\n",
    "    radians = np.deg2rad(df_melted['wind_direction'])\n",
    "    df_melted['wind_direction_x'] = np.cos(radians)\n",
    "    df_melted['wind_direction_y'] = np.sin(radians)\n",
    "    \n",
    "    # Extract date from datetime\n",
    "    df_melted['date'] = df_melted['datetime'].dt.date\n",
    "    \n",
    "    # Group by date and city to get daily mean vectors\n",
    "    result = df_melted.groupby(['date', 'city']).agg({\n",
    "        'wind_direction_x': 'mean',\n",
    "        'wind_direction_y': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(text: str) -> str:\n",
    "    # List of words to remove (can be expanded)\n",
    "    connecting_words = {'with', 'and', 'or', 'the', 'a', 'an', 'in', 'at', 'on'}\n",
    "    \n",
    "    # Replace special characters with spaces\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', str(text))\n",
    "    \n",
    "    # Convert to lowercase and split\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Filter out connecting words\n",
    "    words = [word for word in words if word not in connecting_words]\n",
    "    \n",
    "    # Join with underscore\n",
    "    return '_'.join(words)\n",
    "\n",
    "def get_weather_category_mapping():\n",
    "    # Sky condition mappings\n",
    "    sky_clear = ['sky_is_clear']\n",
    "    sky_partial = ['few_clouds', 'scattered_clouds', 'broken_clouds']\n",
    "    sky_covered = ['overcast_clouds', 'mist', 'fog', 'haze', 'smoke', 'dust', 'sand', 'volcanic_ash']\n",
    "    \n",
    "    # Precipitation mappings\n",
    "    precip_light = [\n",
    "        'light_rain', 'light_snow', 'light_intensity_drizzle', 'drizzle',\n",
    "        'light_intensity_drizzle_rain', 'light_intensity_shower_rain',\n",
    "        'light_rain_snow', 'light_shower_sleet', 'light_shower_snow'\n",
    "    ]\n",
    "    precip_heavy = [\n",
    "        'heavy_intensity_rain', 'very_heavy_rain', 'heavy_intensity_drizzle',\n",
    "        'heavy_intensity_shower_rain', 'heavy_shower_snow', 'heavy_snow',\n",
    "        'shower_rain', 'shower_drizzle', 'shower_snow', 'rain_snow',\n",
    "        'moderate_rain', 'freezing_rain'\n",
    "    ]\n",
    "    \n",
    "    # Storm mappings\n",
    "    storm = [\n",
    "        'thunderstorm', 'heavy_thunderstorm', 'ragged_thunderstorm',\n",
    "        'thunderstorm_drizzle', 'thunderstorm_heavy_drizzle',\n",
    "        'thunderstorm_heavy_rain', 'thunderstorm_light_drizzle',\n",
    "        'thunderstorm_light_rain', 'thunderstorm_rain',\n",
    "        'proximity_thunderstorm', 'proximity_thunderstorm_drizzle',\n",
    "        'proximity_thunderstorm_rain', 'squalls', 'tornado'\n",
    "    ]\n",
    "    \n",
    "    # Create the mapping dictionary\n",
    "    category_mapping = {}\n",
    "    \n",
    "    # Sky condition\n",
    "    for condition in sky_clear:\n",
    "        category_mapping[condition] = 'sky_clear'\n",
    "    for condition in sky_partial:\n",
    "        category_mapping[condition] = 'sky_partial'\n",
    "    for condition in sky_covered:\n",
    "        category_mapping[condition] = 'sky_covered'\n",
    "        \n",
    "    # Precipitation\n",
    "    for condition in precip_light:\n",
    "        category_mapping[condition] = 'precip_light'\n",
    "    for condition in precip_heavy:\n",
    "        category_mapping[condition] = 'precip_heavy'\n",
    "        \n",
    "    # Storm\n",
    "    for condition in storm:\n",
    "        category_mapping[condition] = 'storm'\n",
    "    \n",
    "    return category_mapping\n",
    "\n",
    "def transform_weather_description(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Convert datetime to datetime type if it isn't already\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Get list of city columns (all columns except datetime)\n",
    "    city_columns = [col for col in df.columns if col != 'datetime']\n",
    "    \n",
    "    # Melt the dataframe to convert from wide to long format\n",
    "    df_melted = df.melt(\n",
    "        id_vars=['datetime'],\n",
    "        value_vars=city_columns,\n",
    "        var_name='city',\n",
    "        value_name='class_label'\n",
    "    )\n",
    "    \n",
    "    # Extract date from datetime\n",
    "    df_melted['date'] = df_melted['datetime'].dt.date\n",
    "    \n",
    "    # Convert class labels to snake case\n",
    "    df_melted['class_label'] = df_melted['class_label'].apply(to_snake_case)\n",
    "    \n",
    "    # Map to simplified categories\n",
    "    category_mapping = get_weather_category_mapping()\n",
    "    df_melted['weather_category'] = df_melted['class_label'].map(category_mapping)\n",
    "    \n",
    "    # Create one-hot encoded columns for the simplified categories\n",
    "    one_hot = pd.get_dummies(df_melted['weather_category'], prefix=\"weather_description\")\n",
    "    \n",
    "    # Add one-hot columns to the melted dataframe\n",
    "    df_melted = pd.concat([df_melted[['date', 'city']], one_hot], axis=1)\n",
    "    \n",
    "    # Group by date and city to get daily proportions\n",
    "    result = df_melted.groupby(['date', 'city']).mean().reset_index()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_day_of_year(dates: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode day of year as sin/cos components to capture cyclical nature\n",
    "    \"\"\"\n",
    "    day_of_year = pd.to_datetime(dates).dt.dayofyear\n",
    "    day_of_year_sin = np.sin(2 * np.pi * day_of_year / 365.25)\n",
    "    day_of_year_cos = np.cos(2 * np.pi * day_of_year / 365.25)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'day_of_year_sin': day_of_year_sin,\n",
    "        'day_of_year_cos': day_of_year_cos\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_dict: dict[str, tuple[Path, tuple]], is_training: bool = True) -> list[pd.DataFrame]:\n",
    "    transformed_dfs = []\n",
    "    for measurement_name, args in file_dict.items():\n",
    "        file_path, agg_types = args\n",
    "        if not file_path.exists():\n",
    "            print(f\"File responsible for {measurement_name} doesn't exist at {file_path.absolute()}\")\n",
    "            continue\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        transformed_dfs.append(transform_scalar_data(df, measurement_name, agg_types, is_training=is_training))\n",
    "    return transformed_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_dfs = []\n",
    "transformed_dfs.extend(load_and_preprocess_data(file_dict[\"scalar\"], is_training=True))\n",
    "df_wind_dir = pd.read_csv(DATA_ROOT / \"wind_direction.csv\")\n",
    "transformed_dfs.append(transform_wind_direction(df_wind_dir))\n",
    "df_weather_class = pd.read_csv(DATA_ROOT / \"weather_description.csv\")\n",
    "transformed_dfs.append(transform_weather_description(df_weather_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transformed_dfs[0]\n",
    "    \n",
    "# Merge with each subsequent dataset\n",
    "for df in transformed_dfs[1:]:\n",
    "    result = pd.merge(\n",
    "        result,\n",
    "        df,\n",
    "        on=['date', 'city'],\n",
    "        how='outer'\n",
    "    )\n",
    "\n",
    "# Sort by date and city\n",
    "result = result.sort_values(['date', 'city'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie słownika z oddzielnymi ramkami danych dla każdego miasta\n",
    "city_dfs = {city: df.reset_index(drop=True) for city, df in result.groupby('city')}\n",
    "\n",
    "# Sprawdzenie przykładowego miasta\n",
    "print(\"\\nPrzykładowa ramka danych dla Vancouver:\")\n",
    "print(city_dfs['Vancouver'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja funkcji tworzącej okno przewidywania\n",
    "def create_windows(df: pd.DataFrame, window_size=3, skip=1, scaler_dir: str = \"scalers\"):\n",
    "    with open(scaler_dir + \"/wind_speed_scaler.pkl\", 'rb') as f:\n",
    "        wind_speed_scaler: StandardScaler = pickle.load(f)\n",
    "\n",
    "    wind_speed_limit = wind_speed_scaler.transform([6])[0]\n",
    "\n",
    "    df[\"strong_wind\"] = ((df[\"wind_speed_max\"]) >= wind_speed_limit).astype(int)\n",
    "\n",
    "    X = []\n",
    "    y_temp = []\n",
    "    y_wind = []\n",
    "\n",
    "    for i in range(window_size, len(df) - skip + 1):\n",
    "        # Sprawdzenie czy dni są kolejne\n",
    "        window = df.iloc[i-window_size:i]\n",
    "        target = df.iloc[i + skip -1]\n",
    "\n",
    "        # Sprawdzenie, czy dni są ciągłe\n",
    "        expected_date = window['date'].iloc[-1] + pd.Timedelta(days=1)\n",
    "        if target['date'] != expected_date:\n",
    "            continue\n",
    "        \n",
    "        window = window.drop([\"date\", \"city\", \"wind_speed\"])\n",
    "\n",
    "        # Przygotowanie cech\n",
    "        features = window.values.flatten()\n",
    "        X.append(features)\n",
    "        \n",
    "        y_temp.append()\n",
    "        y_wind.append(target[\"strong_wind\"])\n",
    "\n",
    "    return np.array(X), np.array(y_temp), np.array(y_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SCALERS SHOULD BE CONFIGURED ON TRAINING DATA ONLY AND LOADED FOR TEST DATA\n",
    "# TODO: SCALERS SHOULD BE SEPARATED FROM THE LOAD SCALER DATA FUNCTION\n",
    "# TODO: WRITE AN EASY PIPELINE FOR INFERENCE\n",
    "# TODO: Y_TEMP SHOULD BE PREDICTED WITHIN 2 DEG CELCIUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
